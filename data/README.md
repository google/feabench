Data associated with the benchmark included in the paper.

[**FEABench: Evaluating Language Models on Real-World Physics Reasoning Ability**](https://arxiv.org/abs/2411.xxxxx)

Nayantara Mudur, Hao Cui, Subhashini Venugopalan, Paul Raccuglia, Michael Brenner, Peter Norgaard

## Overview

FEABench is a challenging dataset that seeks to test large language models on their ability to solve problems that require numerical analysis.

### FEABench Gold

This dataset consists of 15 manually verified problems. The data is in the form of json files in the `benchmark.zip` file.

Each problem consists of a natural language description of a physics, engineering or applied mathematics problem as well as a numerical artifact, `target description’ that needs to be computed. The objective of the LLM is to compute the target. Note, although we focus on the ability of LLMs to generate executable COMSOL Multiphysics code to solve these problems, the problem formulations and the `model specifications’ field itself, is software agnostic, i.e. one could potentially call another equally capable numerical package or FEA software to attempt to solve them.

There are 2 possible self-sufficient origins one could potentially use to attempt to generate code:
* ModelSpecs2Code: Model Specifications (+ Selection Information)
* Plan2Code: This is an explicit set of step-by-step instructions to solve the problem in COMSOL.


#### Possible Task Versions

* Solving for the target with \comsol.
  * Input: ModelSpecs+Selection Information
  * Output: Target Value ModelSpecs in the paper
* Translating explicit natural language step to API calls.
  * Input: Plan
  * Output: Ground Truth Code
* Solving for the target with a general numerical software.
  * Input: ModelSpecs
  * Output: Target Value

#### Structure of JSON for FEABench Gold

Each .json file contains the following fields:

* `model_specifications`: A complete description of the FEA task, including geometry, material properties, physics specifications, initial/boundary conditions, and the output to be computed. This field is intended to be general enough \rewrite{to be relevant to softwares or approaches other than} \comsol, yet unambiguous about details such as material properties.
* `selection_information`: An engineer would typically identify spatial information like geometric selections (points, boundaries, and domains) using the Graphical User Interface (GUI). \rewrite{We provide this field as a substitute for images for LLMs and agents without the ability to receive visual input from the GUI.} This information is valid as long as the agent chooses to construct the geometry in a manner that is reasonably similar to the construction of the ground truth (GT) geometry.
* `plan`: Step-by-step instructions to solve the problem using \comsol.
* `target_description`: A brief phrase describing the quantity that needs to be computed.
* `target_value`: The correct value of the target physical quantity.
* `target_units`: The correct units of the physical quantity.
* `ground_truth_code`:, Lines of \comsol API calls that can be executed to build a model that successfully computes the target value.
* `target_tree`: Executing \comsol calls can be regarded as modifying a tree with certain predefined \textit{branches} such as \textit{geometry} and \textit{physics}. The model generated by executing code can thus be represented in a condensed form as a model tree (see \aref{app: example\_bench}). This is a high-level lossy representation of a solution path, as the code cannot be exactly recovered from the model tree.
* `comsol_model_id`: The COMSOL Application Gallery ID that the problem specification is derived from.


